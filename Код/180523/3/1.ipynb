{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.outgoing_connections = []\n",
    "\n",
    "class Connection:\n",
    "    def __init__(self, in_neuron, out_neuron, weight):\n",
    "        self.in_neuron = in_neuron\n",
    "        self.out_neuron = out_neuron\n",
    "        self.weight = weight\n",
    "        self.enabled = True\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, connections, hidden_neurons):\n",
    "        super(Network, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.neurons = []\n",
    "        self.connections = []\n",
    "\n",
    "        for i in range(num_inputs):\n",
    "            self.neurons.append(Neuron(i))\n",
    "\n",
    "        for i in range(num_outputs):\n",
    "            neuron_id = num_inputs + i\n",
    "            self.neurons.append(Neuron(neuron_id))\n",
    "\n",
    "        for conn in connections:\n",
    "            in_neuron = self.neurons[conn.in_neuron]\n",
    "            out_neuron = self.neurons[conn.out_neuron]\n",
    "            connection = Connection(in_neuron, out_neuron, conn.weight)\n",
    "            self.connections.append(connection)\n",
    "            in_neuron.outgoing_connections.append(connection)\n",
    "\n",
    "        for _ in range(hidden_neurons):\n",
    "            neuron_id = len(self.neurons)\n",
    "            self.neurons.append(Neuron(neuron_id))\n",
    "\n",
    "        self.linear = nn.Linear(len(self.neurons), num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.value = 0.0\n",
    "\n",
    "        for i in range(self.num_inputs):\n",
    "            self.neurons[i].value = x[:, i]  # Update to handle batch input\n",
    "\n",
    "        for connection in self.connections:\n",
    "            if connection.enabled:\n",
    "                connection.out_neuron.value += connection.in_neuron.value * connection.weight\n",
    "\n",
    "        output = np.array([])\n",
    "        for i in range(self.num_inputs, self.num_inputs + self.num_outputs):\n",
    "            output = np.append(output, self.neurons[i].value)\n",
    "        output = torch.Tensor(output)\n",
    "        output = self.linear(output)  # Apply linear layer\n",
    "        return torch.sigmoid(output)  # Apply sigmoid activation\n",
    "\n",
    "\n",
    "class NEAT:\n",
    "    def __init__(self, num_inputs, num_outputs, population_size, hidden_neurons):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.population_size = population_size\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.population = []\n",
    "        self.history = {'neurons': [], 'loss': [], 'accuracy': []}\n",
    "        \n",
    "        for _ in range(population_size):\n",
    "            connections = []\n",
    "            for i in range(num_inputs):\n",
    "                for j in range(num_inputs, num_inputs + num_outputs):\n",
    "                    connections.append(Connection(i, j, np.random.uniform(-1, 1)))\n",
    "            \n",
    "            network = Network(num_inputs, num_outputs, connections, hidden_neurons)\n",
    "            self.population.append(network)\n",
    "    \n",
    "    # The remaining code remains the same\n",
    "    \n",
    "    def evaluate(self, network, dataloader, criterion):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                predicted = outputs.round().long()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        return total_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader, test_loader, num_generations, criterion, optimizer):\n",
    "        for generation in range(num_generations):\n",
    "            print(f\"Generation {generation+1}/{num_generations}\")\n",
    "            for network in self.population:\n",
    "                network.train()\n",
    "                loss, accuracy = self.evaluate(network, train_loader, criterion)\n",
    "                self.history['neurons'].append(len(network.neurons))\n",
    "                self.history['loss'].append(loss)\n",
    "                self.history['accuracy'].append(accuracy)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            best_network = max(self.population, key=lambda n: self.evaluate(n, test_loader, criterion)[1])\n",
    "            print(f\"Best Accuracy: {self.evaluate(best_network, test_loader, criterion)[1]}\")\n",
    "            \n",
    "            # Create new population using crossover and mutation\n",
    "            new_population = []\n",
    "            for _ in range(self.population_size):\n",
    "                parent1 = np.random.choice(self.population)\n",
    "                parent2 = np.random.choice(self.population)\n",
    "                child = self.crossover(parent1, parent2)\n",
    "                self.mutate(child)\n",
    "                new_population.append(child)\n",
    "            \n",
    "            self.population = new_population\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        child_connections = []\n",
    "        for conn1, conn2 in zip(parent1.connections, parent2.connections):\n",
    "            if np.random.uniform() < 0.5:\n",
    "                child_connections.append(Connection(conn1.in_neuron.id, conn1.out_neuron.id, conn1.weight))\n",
    "            else:\n",
    "                child_connections.append(Connection(conn2.in_neuron.id, conn2.out_neuron.id, conn2.weight))\n",
    "        \n",
    "        child_hidden_neurons = max(len(parent1.neurons), len(parent2.neurons)) - self.num_inputs - self.num_outputs\n",
    "        child = Network(self.num_inputs, self.num_outputs, child_connections, child_hidden_neurons)\n",
    "        return child\n",
    "    \n",
    "    def mutate(self, network):\n",
    "        for connection in network.connections:\n",
    "            if np.random.uniform() < 0.1:  # Mutate weight\n",
    "                connection.weight += np.random.uniform(-0.1, 0.1)\n",
    "            if np.random.uniform() < 0.01:  # Toggle connection\n",
    "                connection.enabled = not connection.enabled\n",
    "    \n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['neurons'])\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Number of Neurons')\n",
    "        plt.title('Number of Neurons in Hidden Layers')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['loss'], label='Loss')\n",
    "        plt.plot(self.history['accuracy'], label='Accuracy')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Loss and Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 19x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32ma:\\Профиль\\Rab Table\\Учёба\\2\\Neuroevolutionary-computing\\Код\\180523\\3\\1.ipynb Cell 2\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(neat\u001b[39m.\u001b[39mpopulation[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m neat\u001b[39m.\u001b[39;49mtrain(train_loader, test_loader, num_generations, criterion, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m neat\u001b[39m.\u001b[39mplot_history()\n",
      "\u001b[1;32ma:\\Профиль\\Rab Table\\Учёба\\2\\Neuroevolutionary-computing\\Код\\180523\\3\\1.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulation[\u001b[39m0\u001b[39;49m](inputs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\artem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32ma:\\Профиль\\Rab Table\\Учёба\\2\\Neuroevolutionary-computing\\Код\\180523\\3\\1.ipynb Cell 2\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneurons[i]\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(output)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(output)  \u001b[39m# Apply linear layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/%D0%9F%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C/Rab%20Table/%D0%A3%D1%87%D1%91%D0%B1%D0%B0/2/Neuroevolutionary-computing/%D0%9A%D0%BE%D0%B4/180523/3/1.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(output)\n",
      "File \u001b[1;32mc:\\Users\\artem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\artem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 19x1)"
     ]
    }
   ],
   "source": [
    "num_inputs = 8\n",
    "num_outputs = 1\n",
    "population_size = 100\n",
    "hidden_neurons = 10\n",
    "num_generations = 10\n",
    "\n",
    "train_data = np.random.randn(100, num_inputs)\n",
    "train_labels = np.random.randint(0, 2, (100, num_outputs))\n",
    "test_data = np.random.randn(50, num_inputs)\n",
    "test_labels = np.random.randint(0, 2, (50, num_outputs))\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(train_data), torch.Tensor(train_labels))\n",
    "test_dataset = TensorDataset(torch.Tensor(test_data), torch.Tensor(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "neat = NEAT(num_inputs, num_outputs, population_size, hidden_neurons)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(neat.population[0].parameters(), lr=0.001)\n",
    "\n",
    "neat.train(train_loader, test_loader, num_generations, criterion, optimizer)\n",
    "neat.plot_history()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
